{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57084ea9-cf88-4dad-902e-8f93479bf0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e20fc0f-9601-4d64-9173-cccd54ba4ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1. Device configuration ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d3e26fc-1032-4b63-8f3a-fe3e43e0ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2 Define ImageNet preprocessing ===\n",
    "mean_norms = np.array([0.485, 0.456, 0.406])\n",
    "std_norms = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean_norms, std=std_norms)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97b3362c-b769-4e22-b1bf-2588211adddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3。 Function to reload dataset with corrected label mapping ===\n",
    "def reload_testset_after_fix(dataset_path, transform, fixed_class_to_idx):\n",
    "    \"\"\"\n",
    "    Reload the ImageFolder dataset and replace the class_to_idx and samples\n",
    "    to reflect correct ImageNet indices based on synset IDs.\n",
    "    \"\"\"\n",
    "    # Reload the dataset\n",
    "    testset = ImageFolder(root=dataset_path, transform=transform)\n",
    "\n",
    "    # Replace class_to_idx with the fixed mapping\n",
    "    testset.class_to_idx = fixed_class_to_idx\n",
    "\n",
    "    # Manually reassign each sample’s label using the new mapping\n",
    "    new_samples = []\n",
    "    for path, _ in testset.samples:\n",
    "        class_name = os.path.basename(os.path.dirname(path))\n",
    "        if class_name in fixed_class_to_idx:\n",
    "            new_samples.append((path, fixed_class_to_idx[class_name]))\n",
    "        else:\n",
    "            print(f\"⚠️ Skipping unmatched class: {class_name}\")\n",
    "    testset.samples = new_samples\n",
    "\n",
    "    testloader = DataLoader(testset, batch_size=16, shuffle=False)\n",
    "\n",
    "    print(\"✅ Fully refreshed dataset with correct label indices:\")\n",
    "    for i in range(3):\n",
    "        _, label = testset[i]\n",
    "        print(f\"Image {i} label index: {label}\")\n",
    "\n",
    "    return testset, testloader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d4694b2-a9dc-4a53-b81a-f4bf3db708bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4. Build the corrected synset → index mapping ===\n",
    "dataset_path = r\"C:\\Users\\AthrunZala\\Downloads\\TestDataSet\"\n",
    "filtered_path = os.path.join(dataset_path, \"filtered_classes\")\n",
    "\n",
    "# Load dataset to get original class names\n",
    "testset_raw = ImageFolder(root=filtered_path, transform=transform)\n",
    "\n",
    "# Load official ImageNet synset-to-index mapping\n",
    "with open(os.path.join(dataset_path, \"imagenet_class_index.json\"), \"r\") as f:\n",
    "    imagenet_class_index = json.load(f)\n",
    "synset_to_idx = {v[0]: int(k) for k, v in imagenet_class_index.items()}\n",
    "\n",
    "# Match synset folders to correct ImageNet index\n",
    "fixed_class_to_idx = {\n",
    "    synset: synset_to_idx[synset]\n",
    "    for synset in testset_raw.class_to_idx\n",
    "    if synset in synset_to_idx\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51618ed2-3e97-4786-b1c7-aaac92315afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fully refreshed dataset with correct label indices:\n",
      "Image 0 label index: 401\n",
      "Image 1 label index: 401\n",
      "Image 2 label index: 401\n"
     ]
    }
   ],
   "source": [
    "# === 5. Refresh dataset using corrected mapping ===\n",
    "testset, testloader = reload_testset_after_fix(\n",
    "    dataset_path=filtered_path,\n",
    "    transform=transform,\n",
    "    fixed_class_to_idx=fixed_class_to_idx\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7eb1ea52-c545-42e7-b2a4-cd201ad47453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 6. Load pre-trained ResNet-34 ===\n",
    "model = models.resnet34(weights='IMAGENET1K_V1').to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82dbbae6-8bf7-4202-a4c2-9a7bb336f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 7. Accuracy calculation ===\n",
    "def accuracy(output, target, topk=(1, 5)):\n",
    "    maxk = max(topk)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    correct = pred.eq(target.view(-1, 1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:, :k].sum().item()\n",
    "        res.append(correct_k)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccf87612-3ab0-4603-9c7e-51b7980469f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader):\n",
    "    model.eval()\n",
    "    top1_total = 0\n",
    "    top5_total = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            top1, top5 = accuracy(outputs, labels)\n",
    "            top1_total += top1\n",
    "            top5_total += top5\n",
    "            total += labels.size(0)\n",
    "    return top1_total / total, top5_total / total\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5cf556b-a963-4f2b-9fc7-24ac59ae511d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 8. FGSM attack ===\n",
    "def fgsm_attack(model, images, labels, epsilon):\n",
    "    images = images.clone().detach().requires_grad_()\n",
    "    outputs = model(images)\n",
    "    loss = F.cross_entropy(outputs, labels)\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    perturbed = images + epsilon * images.grad.sign()\n",
    "    return torch.clamp(perturbed, 0, 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e0ca31c-6c9b-461f-9de3-47012d2c77b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 9. PGD attack ===\n",
    "def pgd_attack(model, images, labels, epsilon=0.02, alpha=0.005, steps=10):\n",
    "    ori_images = images.clone().detach()\n",
    "    perturbed = images.clone().detach()\n",
    "    for _ in range(steps):\n",
    "        perturbed.requires_grad = True\n",
    "        outputs = model(perturbed)\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        grad = perturbed.grad.detach()\n",
    "        perturbed = perturbed + alpha * grad.sign()\n",
    "        perturbed = torch.min(torch.max(perturbed, ori_images - epsilon), ori_images + epsilon)\n",
    "        perturbed = torch.clamp(perturbed, 0, 1).detach()\n",
    "    return perturbed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a50593fa-6fc2-4d2f-85a8-6a5726c1f19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 10. Patch attack ===\n",
    "def patch_attack(model, images, labels, epsilon=0.5, patch_size=32):\n",
    "    attacked = images.clone().detach()\n",
    "    for i in range(images.size(0)):\n",
    "        x, y = np.random.randint(0, 224 - patch_size + 1, 2)\n",
    "        patch = attacked[i:i+1, :, y:y+patch_size, x:x+patch_size]\n",
    "        adv_patch = fgsm_attack(model, patch.clone(), labels[i:i+1], epsilon)\n",
    "        attacked[i, :, y:y+patch_size, x:x+patch_size] = adv_patch\n",
    "    return attacked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c278a6f1-c193-4f73-93ea-40c98ca238fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 11. Visualize original vs adversarial ===\n",
    "def visualize(original, adversarial, filename):\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=(-mean_norms / std_norms).tolist(),\n",
    "        std=(1 / std_norms).tolist()\n",
    "    )\n",
    "    orig = inv_normalize(original.detach().cpu()).permute(1, 2, 0).numpy()\n",
    "    adv = inv_normalize(adversarial.detach().cpu()).permute(1, 2, 0).numpy()\n",
    "    fig, ax = plt.subplots(1, 2)\n",
    "    ax[0].imshow(np.clip(orig, 0, 1))\n",
    "    ax[0].set_title(\"Original\")\n",
    "    ax[1].imshow(np.clip(adv, 0, 1))\n",
    "    ax[1].set_title(\"Adversarial\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dd058da-928b-4b47-83d0-6301fc1a7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 12. Run full pipeline ===\n",
    "def run_all():\n",
    "    print(\"=== Task 1: Original Accuracy ===\")\n",
    "    top1, top5 = evaluate(model, testloader)\n",
    "    print(f\"Top-1: {top1:.4f}, Top-5: {top5:.4f}\")\n",
    "\n",
    "    print(\"=== Task 2: FGSM Attack ===\")\n",
    "    fgsm_images, labels = [], []\n",
    "    for images, targets in tqdm(testloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        adv = fgsm_attack(model, images, targets, epsilon=0.02)\n",
    "        fgsm_images.append(adv.cpu())\n",
    "        labels.append(targets.cpu())\n",
    "        visualize(images[0], adv[0], \"fgsm_example.png\")\n",
    "    adv_dataset1 = torch.utils.data.TensorDataset(torch.cat(fgsm_images), torch.cat(labels))\n",
    "    adv_loader1 = DataLoader(adv_dataset1, batch_size=16)\n",
    "    top1_fgsm, top5_fgsm = evaluate(model, adv_loader1)\n",
    "    print(f\"FGSM Top-1: {top1_fgsm:.4f}, Top-5: {top5_fgsm:.4f}\")\n",
    "\n",
    "    print(\"=== Task 3: PGD Attack ===\")\n",
    "    pgd_images = []\n",
    "    for images, targets in tqdm(testloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        adv = pgd_attack(model, images, targets)\n",
    "        pgd_images.append(adv.cpu())\n",
    "        visualize(images[0], adv[0], \"pgd_example.png\")\n",
    "    adv_dataset2 = torch.utils.data.TensorDataset(torch.cat(pgd_images), torch.cat(labels))\n",
    "    adv_loader2 = DataLoader(adv_dataset2, batch_size=16)\n",
    "    top1_pgd, top5_pgd = evaluate(model, adv_loader2)\n",
    "    print(f\"PGD Top-1: {top1_pgd:.4f}, Top-5: {top5_pgd:.4f}\")\n",
    "\n",
    "    print(\"=== Task 4: Patch Attack ===\")\n",
    "    patch_images = []\n",
    "    for images, targets in tqdm(testloader):\n",
    "        images, targets = images.to(device), targets.to(device)\n",
    "        adv = patch_attack(model, images, targets, epsilon=0.5)\n",
    "        patch_images.append(adv.cpu())\n",
    "        visualize(images[0], adv[0], \"patch_example.png\")\n",
    "    adv_dataset3 = torch.utils.data.TensorDataset(torch.cat(patch_images), torch.cat(labels))\n",
    "    adv_loader3 = DataLoader(adv_dataset3, batch_size=16)\n",
    "    top1_patch, top5_patch = evaluate(model, adv_loader3)\n",
    "    print(f\"Patch Top-1: {top1_patch:.4f}, Top-5: {top5_patch:.4f}\")\n",
    "\n",
    "    print(\"=== Task 5: Transferability to DenseNet-121 ===\")\n",
    "    densenet = models.densenet121(weights='IMAGENET1K_V1').to(device).eval()\n",
    "    accs = {\n",
    "        \"Original\": evaluate(densenet, testloader),\n",
    "        \"FGSM\": evaluate(densenet, adv_loader1),\n",
    "        \"PGD\": evaluate(densenet, adv_loader2),\n",
    "        \"Patch\": evaluate(densenet, adv_loader3)\n",
    "    }\n",
    "    for name, (t1, t5) in accs.items():\n",
    "        print(f\"{name}: Top-1 = {t1:.4f}, Top-5 = {t5:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d82a4d56-854f-45f9-910f-b54a18ceb9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Task 1: Original Accuracy ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:13<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top-1: 0.7600, Top-5: 0.9420\n",
      "=== Task 2: FGSM Attack ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:24<00:00,  1.31it/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 40.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FGSM Top-1: 0.2640, Top-5: 0.5020\n",
      "=== Task 3: PGD Attack ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:50<00:00,  1.59s/it]\n",
      "100%|██████████| 32/32 [00:01<00:00, 23.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PGD Top-1: 0.0040, Top-5: 0.0660\n",
      "=== Task 4: Patch Attack ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:30<00:00,  1.05it/s]\n",
      "100%|██████████| 32/32 [00:01<00:00, 29.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patch Top-1: 0.7520, Top-5: 0.9380\n",
      "=== Task 5: Transferability to DenseNet-121 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:02<00:00, 10.86it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 14.10it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 11.57it/s]\n",
      "100%|██████████| 32/32 [00:02<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Top-1 = 0.7460, Top-5 = 0.9360\n",
      "FGSM: Top-1 = 0.4260, Top-5 = 0.6620\n",
      "PGD: Top-1 = 0.3980, Top-5 = 0.6280\n",
      "Patch: Top-1 = 0.7380, Top-5 = 0.9220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    run_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c614739a-5b16-456c-9d0e-8f05f0a645d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (clip_hw3_plus)",
   "language": "python",
   "name": "clip_hw3_plus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
